# 분봉 데이터 수집 문제 해결

## 문제 원인

### 네이버 금융 데이터 제공 방식

네이버 금융은 분봉 데이터를 **최신 시간(15:30 근처)부터 역순**으로 제공합니다:

- 페이지 1: 15:30 근처 데이터 (최신)
- 페이지 2: 15:20 근처 데이터
- 페이지 3: 15:10 근처 데이터
- ...
- 페이지 39: 09:00 근처 데이터 (장 시작)

### 필요한 페이지 수 계산

- **장 시작**: 09:00
- **장 종료**: 15:30
- **총 시간**: 6시간 30분 = 390분
- **분봉 개수**: 약 390개 (1분 단위)
- **페이지당 데이터**: 약 10개
- **필요한 페이지 수**: 약 39페이지

### 이전 문제

- **기본 페이지 수**: 20페이지
- **수집 가능한 데이터**: 약 200개 (15:30 ~ 11:30 정도)
- **결과**: 장 시작 시간(09:00) 데이터를 놓침

---

## 해결 방법

### 1. 기본 페이지 수 증가

```python
# 이전
pages = 20

# 개선 후
pages = 40  # 장 시작부터 수집 가능
```

### 2. 자동 페이지 수 증가

`pages < 40`이면 자동으로 40으로 증가:

```python
if pages < 40:
    logger.warning(f"pages={pages}는 장 시작(09:00) 데이터 수집에 부족할 수 있습니다. 40으로 증가합니다.")
    pages = 40
```

### 3. 장 시작 시간 확인 및 추가 수집

수집 후 첫 데이터 시간을 확인하고, 09:00 이전이면 추가 수집:

```python
if intraday_data:
    first_datetime = intraday_data[0]['datetime']
    first_hour = first_datetime.hour
    first_minute = first_datetime.minute
    
    # 장 시작 시간(09:00) 이전 데이터가 있으면 더 많은 페이지 필요
    if first_hour > 9 or (first_hour == 9 and first_minute > 0):
        # 추가로 20페이지 더 수집
        additional_data = self.fetch_intraday_data(ticker, pages + 20, actual_date)
        # 중복 제거 후 병합
```

---

## 변경 사항

### 백엔드

1. **`backend/app/services/intraday_collector.py`**
   - `collect_and_save_intraday`: 기본값이 10이면 40으로 자동 증가
   - 장 시작 시간(09:00) 데이터 확인 및 추가 수집 로직 추가
   - 마지막 거래일 데이터에도 동일 로직 적용

2. **`backend/app/routers/etfs.py`**
   - 자동 수집: `pages=20` → `pages=40`
   - 수동 수집 API: 기본값 `pages=20` → `pages=40`

---

## 수집 흐름

```
1. 기본 페이지 수로 수집 (40페이지)
   ↓
2. 첫 데이터 시간 확인
   ↓
3. 09:00 이전이면?
   → 추가로 20페이지 더 수집
   → 중복 제거 후 병합
   ↓
4. DB 저장
```

---

## 예상 결과

### 이전
- 수집 데이터: 약 200개 (15:30 ~ 11:30)
- 장 시작 데이터: ❌ 누락

### 개선 후
- 수집 데이터: 약 390개 (15:30 ~ 09:00)
- 장 시작 데이터: ✅ 포함

---

## 참고

- 네이버 금융은 최신 데이터부터 역순 제공
- 페이지당 약 10개 데이터
- 장 시작부터 수집하려면 최소 40페이지 필요
- 더 안전하게 하려면 50페이지까지 가능 (API 제한: 50페이지)
